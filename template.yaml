AWSTemplateFormatVersion: "2010-09-09"
Description: Deploy the slurm infrastructure required to run the SAGAsign bioinformatics workflow

Parameters:
  SlurmPackageUrl:
    Type: String
    Default: https://download.schedmd.com/slurm/slurm-23.02.2.tar.bz2
    Description: URL to the Slurm installation package. The filename must be like slurm-*.tar.bz2

  PluginPrefixUrl:
    Type: String
    Default: https://github.com/ckrushton/aws-plugin-slurm-hibernate/raw/hibernate-support/
    Description: Path to the plugin files

  VpcId:
    Type: AWS::EC2::VPC::Id
    Description: VPC where the headnode and the compute nodes will be launched

  Subnet1Id:
    Type: AWS::EC2::Subnet::Id
    Description: Subnet in the VPC where the headnode and the compute nodes will be launched. Must be a public subnet or a private subnet with a default route to NAT

  KeyPair:
    Type: AWS::EC2::KeyPair::KeyName
    Description: Key pair that will be used to launch instances

  LatestAmiId:
    Type: String
    Default: ami-07af57b22c5433af3
    Description: AMI to be used for head node and cluster node creation

  HeadNodeInstanceType:
    Type: String
    Default: t3.2xlarge
    Description: Instance type to use to launch the head node. If the instance has attatched storage, this will be configured as a shared network drive.

Metadata: 
  AWS::CloudFormation::Interface: 
    ParameterGroups: 
      - Label: 
          default: Network
        Parameters: 
          - VpcId
          - Subnet1Id
      - Label: 
          default: Instances
        Parameters: 
          - HeadNodeInstanceType
          - ComputeNodeInstanceType
          - ComputeNodeCPUs
          - KeyPair
          - LatestAmiId
      - Label: 
          default: Packages
        Parameters: 
          - SlurmPackageUrl
          - PluginPrefixUrl
    ParameterLabels: 
      VpcId: 
        default: VPC ID
      Subnet1Id: 
        default: Subnet 1 ID
      HeadNodeInstanceType: 
        default: Headnode Instance Type
      KeyPair: 
        default: Key Pair
      LatestAmiId: 
        default: Latest Amazon Linux 2 AMI ID
      SlurmPackageUrl: 
        default: Slurm Package URL
      PluginPrefixUrl: 
        default: Plugin URL Prefix

Resources:
  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
        GroupDescription: Allow SSH traffic from Internet and traffic between Slurm nodes
        VpcId: !Ref VpcId
        SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0

  SecurityGroupInbound:
      Type: AWS::EC2::SecurityGroupIngress
      Properties:
        IpProtocol: -1
        SourceSecurityGroupId: !GetAtt [ SecurityGroup, GroupId ]
        GroupId: !GetAtt [ SecurityGroup, GroupId ]

  ComputeNodeRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - ec2:DescribeTags
                  - fsx:DescribeFileSystems
                Resource: '*'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

  ComputeNodeInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref ComputeNodeRole

  HeadNodeRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - ec2:CreateFleet
                  - ec2:DescribeFleets
                  - ec2:TerminateFleets
                  - ec2:DescribeFleetInstances
                  - ec2:RunInstances
                  - ec2:TerminateInstances
                  - ec2:ModifyFleet
                  - ec2:CreateTags
                  - ec2:DescribeInstances
                  - ec2:DescribeInstanceTypes
                  - ec2:DescribeTags
                  - iam:CreateServiceLinkedRole
                  - fsx:DescribeFileSystems
                  - s3:GetObject
                  - s3:GetObjectAttributes
                  - s3:ListBucket
                  - s3:PutObject
                Resource: '*'
              - Effect: Allow
                Action: iam:PassRole
                Resource: !GetAtt [ ComputeNodeRole, Arn ]
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

  HeadNodeInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref HeadNodeRole

  CleanupRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "lambda.amazonaws.com"
            Action: sts:AssumeRole
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - ec2:DescribeFleets
                  - ec2:TerminateFleets
                  - ec2:TerminateInstances
                  - logs:CreateLogStream
                  - logs:PutLogEvent
                Resource: '*'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

  CleanupRoleProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref CleanupRole

  HeadNodeNetworkInterface:
    Type: AWS::EC2::NetworkInterface
    Properties: 
      GroupSet: 
        - !GetAtt [ SecurityGroup, GroupId ]
      SubnetId: !Ref Subnet1Id

  LaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateData:
        KeyName: !Ref KeyPair
        ImageId: !Ref LatestAmiId
        IamInstanceProfile:
          Arn: !GetAtt [ ComputeNodeInstanceProfile, Arn ]
        MetadataOptions:
          InstanceMetadataTags: enabled
        SecurityGroupIds:
          - !GetAtt [ SecurityGroup, GroupId ]
        UserData:
          Fn::Base64:
            !Sub |
              #!/bin/bash -x

              sudo rm /lib/systemd/system/nfs-common.service
              systemctl daemon-reload
              systemctl enable nfs-common
              systemctl start nfs-common
              mkdir -p /nfs
              mkdir -p /shared
              sudo chmod 777 /shared/
              mount -t nfs ${HeadNodeNetworkInterface.PrimaryPrivateIpAddress}:/nfs /nfs
              mount -t nfs ${HeadNodeNetworkInterface.PrimaryPrivateIpAddress}:/shared /shared
              export SLURM_HOME=/nfs/slurm
              sleep 10

              mkdir -p /var/spool/slurm
              'cp' $SLURM_HOME/etc/slurm/slurmd.service /lib/systemd/system
              systemctl enable slurmd.service
              systemctl start slurmd.service

  HeadNodeInstance:
    Type: AWS::EC2::Instance
    Properties: 
      ImageId: !Ref LatestAmiId
      InstanceType: !Ref HeadNodeInstanceType
      IamInstanceProfile: !Ref HeadNodeInstanceProfile
      KeyName:  !Ref KeyPair
      NetworkInterfaces: 
        - NetworkInterfaceId: !Ref HeadNodeNetworkInterface
          DeviceIndex: 0
      Tags:
        - Key: Name
          Value: headnode
      UserData:
          Fn::Base64:
            !Sub |
              #!/bin/bash -x

              # Configure local storage (if availible).
              mountpoint=/shared/
              mkdir -p $mountpoint
              chmod 777 $mountpoint
              fdisk -l
              localdisks=($(fdisk -l | grep -o '/dev/nvme[0-9]*n[0-9]*' | grep -v nvme0))

              if [[ ${!#localdisks[*]} -gt 0 ]]; then
                # Local storage present. Do we have multiple drives?
                if [[ ${!#localdisks[*]} -gt 1 ]]; then
                  # Multiple drives. Stripe together as RAID 0.
                  mdadm --create --verbose /dev/md0 --level 0 --raid-devices ${!#localdisks[*]} ${!localdisks[*]}
                  mountdevice="/dev/md0"
                else
                  # Single device.
                  mountdevice=${!localdisks[0]}
                fi
                # Create filesystem and mount
                mkfs.ext4 -F $mountdevice
                mount $mountdevice $mountpoint
              fi

              # Configure NFS share
              mkdir -p /nfs
              echo "/nfs *(rw,async,no_subtree_check,no_root_squash)" | tee /etc/exports
              echo "$mountpoint *(rw,async,no_subtree_check,no_root_squash)" | tee --append /etc/exports
              systemctl enable nfs-server
              systemctl start nfs-server
              exportfs -av

              # As restoring from an EBS snapshot doesn't actually "restore" the files (until they are read)
              # force read essential files quickly to ensure they are cached.
              sudo apt install -y parallel
              reffiles="/home/sagasign-resources/references/"
              find $reffiles -type f | parallel --jobs 8 dd if={} of=/dev/null bs=10M iflag=direct &

              # Install Slurm
              cd /home/ubuntu/
              wget -q ${SlurmPackageUrl}
              tar -xvf /home/ubuntu/slurm-*.tar.bz2 -C /home/ubuntu
              cd /home/ubuntu/slurm-*/
              /home/ubuntu/slurm-*/configure --prefix=/nfs/slurm
              make -j 12
              make install -j 12
              make install-contrib -j 12
              sleep 5
              export SLURM_HOME=/nfs/slurm
              mkdir -p $SLURM_HOME/etc/slurm
              'cp' /home/ubuntu/slurm-*/etc/* $SLURM_HOME/etc/slurm

              # Install plugin
              mkdir -p $SLURM_HOME/etc/aws
              cd $SLURM_HOME/etc/aws
              wget -q ${PluginPrefixUrl}common.py
              wget -q ${PluginPrefixUrl}resume.py
              wget -q ${PluginPrefixUrl}suspend.py
              wget -q ${PluginPrefixUrl}generate_conf.py
              wget -q ${PluginPrefixUrl}change_state.py
              wget -q ${PluginPrefixUrl}fleet_daemon.py 
              chmod +x *.py

              cat > $SLURM_HOME/etc/aws/config.json <<EOF
              {
                 "LogLevel": "DEBUG",
                 "NodePartitionFolder": "/nfs/slurm/etc/aws/partitions/",
                 "LogFileName": "/var/log/slurm_plugin.log",
                 "SlurmBinPath": "$SLURM_HOME/bin",
                 "SlurmConf": {
                    "PrivateData": "CLOUD",
                    "ResumeProgram": "$SLURM_HOME/etc/aws/resume.py",
                    "SuspendProgram": "$SLURM_HOME/etc/aws/suspend.py",
                    "ResumeRate": 100,
                    "SuspendRate": 100,
                    "ResumeTimeout": 480,
                    "SuspendTime": 350,
                    "TreeWidth": 60000,
                    "GresTypes": "gres"
                 }
              }
              EOF

              cat > $SLURM_HOME/etc/aws/partitions.json <<'EOF'
              {
                 "Partitions": [
                    {
                       "PartitionName": "compm6i",
                       "NodeGroups": [
                          {
                             "NodeGroupName": "node",
                             "MaxNodes": 100,
                             "Region": "${AWS::Region}",
                             "PurchasingOption": "spot",
                             "SpotOptions": {
                                 "AllocationStrategy": "lowest-price",
                                 "InstanceInterruptionBehavior": "hibernate"
                             },
                             "LaunchTemplateSpecification": {
                                "LaunchTemplateId": "${LaunchTemplate}",
                                "Version": "$Latest"
                             },
                             "LaunchTemplateOverrides": [
                                {
                                   "InstanceType": "m6i.4xlarge"
                                }
                             ],
                             "SubnetIds": [
                                "${Subnet1Id}"
                             ]
                          }
                       ],
                       "PartitionOptions": {
                          "Default": "Yes"
                       }
                    }, 
                    {
                       "PartitionName": "large",
                       "NodeGroups": [
                          {
                             "NodeGroupName": "node",
                             "MaxNodes": 100,
                             "Region": "${AWS::Region}",
                             "PurchasingOption": "spot",
                             "SpotOptions": {
                                 "AllocationStrategy": "lowest-price",
                                 "InstanceInterruptionBehavior": "hibernate"
                             },
                             "LaunchTemplateSpecification": {
                                "LaunchTemplateId": "${LaunchTemplate}",
                                "Version": "$Latest"
                             },
                             "LaunchTemplateOverrides": [
                                {
                                    "InstanceType": "c5.12xlarge",
                                    "Priority": 3.0
                                },
                                {
                                    "InstanceType": "c6i.12xlarge",
                                    "Priority": 1.0
                                },
                                {
                                    "InstanceType": "c7i.12xlarge",
                                    "Priority": 2.0
                                }
                             ],
                             "SubnetIds": [
                                "${Subnet1Id}"
                             ]
                          }
                       ],
                       "PartitionOptions": {
                          "Default": "Yes"
                       }
                    },
                    {
                       "PartitionName": "xlarge",
                       "NodeGroups": [
                          {
                             "NodeGroupName": "node",
                             "MaxNodes": 100,
                             "Region": "${AWS::Region}",
                             "PurchasingOption": "on-demand",
                             "SpotOptions": {
                                 "AllocationStrategy": "lowest-price",
                                 "InstanceInterruptionBehavior": "terminate"
                             },
                             "LaunchTemplateSpecification": {
                                "LaunchTemplateId": "${LaunchTemplate}",
                                "Version": "$Latest"
                             },
                             "LaunchTemplateOverrides": [
                                {
                                   "InstanceType":  "c6i.32xlarge",
                                   "Priority": 1.0
                                }
                             ],
                             "SubnetIds": [
                                "${Subnet1Id}"
                             ]
                          }
                       ],
                       "PartitionOptions": {
                          "Default": "Yes"
                       }
                    }
                 ]
              }
              EOF

              cat > $SLURM_HOME/etc/slurm.conf <<'EOF'
              ClusterName=amazon
              ControlMachine=@HEADNODE@
              ControlAddr=${HeadNodeNetworkInterface.PrimaryPrivateIpAddress}
              SlurmdUser=root
              SlurmctldPort=6817
              SlurmdPort=6818
              AuthType=auth/munge
              StateSaveLocation=/var/spool/slurm/ctld
              SlurmdSpoolDir=/var/spool/slurm/d
              SwitchType=switch/none
              MpiDefault=none
              SlurmctldPidFile=/var/run/slurmctld.pid
              SlurmdPidFile=/var/run/slurmd.pid
              ProctrackType=proctrack/pgid
              ReturnToService=2
              # TIMERS
              SlurmctldTimeout=300
              SlurmdTimeout=60
              InactiveLimit=0
              MinJobAge=300
              KillWait=30
              Waittime=0
              # SCHEDULING
              SchedulerType=sched/backfill
              SelectType=select/cons_res
              SelectTypeParameters=CR_Core_Memory
              DefMemPerCpu=100
              # LOGGING
              SlurmctldDebug=8
              SlurmctldLogFile=/var/log/slurmctld.log
              SlurmdDebug=8
              SlurmdLogFile=/var/log/slurmd.log
              DebugFlags=NO_CONF_HASH
              JobCompType=jobcomp/none
              EOF
              sed -i "s|@HEADNODE@|$HOSTNAME|g" $SLURM_HOME/etc/slurm.conf

              # Configure the plugin
              mkdir -p $SLURM_HOME/etc/aws/partitions/
              cd $SLURM_HOME/etc/aws/
              $SLURM_HOME/etc/aws/generate_conf.py ${AWS::StackId}
              cat $SLURM_HOME/etc/aws/slurm.conf.aws >> $SLURM_HOME/etc/slurm.conf
              cp $SLURM_HOME/etc/aws/gres.conf.aws $SLURM_HOME/etc/gres.conf

              crontab -l > mycron
              cat > mycron <<EOF
              * * * * * $SLURM_HOME/etc/aws/change_state.py &>/dev/null
              * * * * * $SLURM_HOME/etc/aws/fleet_daemon.py &>/dev/null
              EOF
              crontab mycron
              rm mycron

              # Get dynamically the node name when launching slurmd
              cat > $SLURM_HOME/etc/aws/get_nodename <<'EOF'
              instanceid=`/usr/bin/curl --fail -m 2 -s http://169.254.169.254/latest/meta-data/instance-id`
              if [[ ! -z "$instanceid" ]]; then
                 hostname=`aws ec2 describe-tags --region ${AWS::Region} --filters "Name=resource-id,Values=$instanceid" "Name=key,Values=Name" --output text | cut -f5`
              fi
              if [ ! -z "$hostname" -a "$hostname" != "None" ]; then
                 echo $hostname
              else
                 echo `hostname`
              fi
              EOF
              chmod +x $SLURM_HOME/etc/aws/get_nodename

              cat > $SLURM_HOME/etc/slurm/slurmd.service <<EOF
              [Unit]
              Description=Slurm node daemon
              After=munge.service network.target remote-fs.target
              [Service]
              Type=forking
              EnvironmentFile=-/etc/sysconfig/slurmd
              ExecStartPre=/bin/bash -c "/bin/systemctl set-environment SLURM_NODENAME=\$($SLURM_HOME/etc/aws/get_nodename)"
              ExecStart=/nfs/slurm/sbin/slurmd -N \$SLURM_NODENAME \$SLURMD_OPTIONS
              ExecReload=/bin/kill -HUP \$MAINPID
              PIDFile=/var/run/slurmd.pid
              KillMode=process
              LimitNOFILE=131072
              LimitMEMLOCK=infinity
              LimitSTACK=infinity
              Delegate=yes
              [Install]
              WantedBy=multi-user.target
              EOF
              
              cat > $SLURM_HOME/etc/slurm/slurmctld.service <<EOF
              [Unit]
              Description=Slurm controller daemon
              After=network.target munge.service
              ConditionPathExists=/nfs/slurm/etc/slurm.conf
              [Service]
              Type=forking
              EnvironmentFile=-/etc/sysconfig/slurmctld
              ExecStart=/nfs/slurm/sbin/slurmctld \$SLURMCTLD_OPTIONS
              ExecReload=/bin/kill -HUP \$MAINPID
              PIDFile=/var/run/slurmctld.pid
              LimitNOFILE=65536
              [Install]
              WantedBy=multi-user.target
              EOF

              cat > $SLURM_HOME/etc/cgroup.conf <<EOF
              AllowedRAMSpace=100.0
              AllowedSwapSpace=0.0
              ConstrainRAMSpace=yes
              ConstrainSwapSpace=yes
              MemorySwappiness=0
              CgroupAutomount=yes
              ConstrainCores=yes
              EOF


              # Set environment variables
              echo 'export SLURM_HOME=/nfs/slurm' | tee /etc/profile.d/slurm.sh
              echo 'export SLURM_CONF=$SLURM_HOME/etc/slurm.conf' | tee -a /etc/profile.d/slurm.sh
              echo 'export PATH=/nfs/slurm/bin:$PATH' | tee -a /etc/profile.d/slurm.sh

              # Launch Slurmctld
              mkdir -p /var/spool/slurm
              'cp' /nfs/slurm/etc/slurm/slurmd.service /lib/systemd/system
              'cp' /nfs/slurm/etc/slurm/slurmctld.service /lib/systemd/system
              systemctl enable slurmctld
              systemctl start slurmctld

              # Add slurm to bashrc file
              echo export PATH=$SLURM_HOME/bin:'$PATH' >> /home/ubuntu/.bashrc

              cd /home/ubuntu
              pip3 install https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-py3-latest.tar.gz
              pip install boto3 filelock

              # Sync to shared filesystem
              mkdir -p /shared/sagasign-resources/references/
              chmod 777 /shared/sagasign-resources/references/
              cp -r /home/sagasign-resources/references/* /shared/sagasign-resources/references/ &

              cfn-signal -e $? --stack ${AWS::StackName} --resource HeadNodeInstance --region ${AWS::Region}
    CreationPolicy:
      ResourceSignal:
        Timeout: PT20M

  CleanupResources:
    Type: AWS::Lambda::Function
    Properties:
      Role:
        Fn::GetAtt:
          - CleanupRole
          - Arn
      Code:
        ZipFile:
          Fn::Join:
            - ""
            - - !Ref PluginPrefixUrl
              - "lambda_cleanup_func.zip"
      FunctionName:
        Fn::Join:
        - ''
        - - CleanupResources-
          - Fn::Select:
            - 2
            - Fn::Split:
              - /
              - Ref: AWS::StackId
      Handler: cleanup_function.handler
      MemorySize: 128
      Runtime: python3.9
      Timeout: 60

Outputs:
  HeadNodeId:
    Description: Head node instance ID
    Value: !Ref HeadNodeInstance
