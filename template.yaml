AWSTemplateFormatVersion: "2010-09-09"
Description: Deploy the slurm infrastructure required to run the SAGAsign bioinformatics workflow

Parameters:
  VpcId:
    Type: AWS::EC2::VPC::Id
    Description: VPC where the headnode and the compute nodes will be launched

  SubnetId:
    Type: AWS::EC2::Subnet::Id
    Description: Subnet in the VPC where the headnode and the compute nodes will be launched. Must be a public subnet or a private subnet with a default route to NAT

  KeyPair:
    Type: AWS::EC2::KeyPair::KeyName
    Description: Key pair that will be used to launch instances

  LatestAmiId:
    Type: String
    Default: ami-0bba1c902eb2316e8
    Description: AMI to be used for head node and cluster node creation

  HeadNodeInstanceType:
    Type: String
    Default: i3en.6xlarge
    Description: Instance type to use to launch the head node. If the instance has attatched storage, this will be configured as a shared network drive.

  NodeHibernateMaxTime:
    Type: Number
    Default: "1800"
    Description: How many seconds should Slurm wait for a hibernated node to resume before requesting a replacement node? Set to 0 for infinite.

Metadata: 
  AWS::CloudFormation::Interface: 
    ParameterGroups: 
      - Label: 
          default: Network
        Parameters: 
          - VpcId
          - SubnetId
      - Label: 
          default: Instances
        Parameters: 
          - HeadNodeInstanceType
          - KeyPair
          - LatestAmiId
          - NodeHibernateMaxTime
    ParameterLabels: 
      VpcId: 
        default: VPC ID
      SubnetId: 
        default: Subnet ID
      HeadNodeInstanceType: 
        default: Headnode Instance Type
      KeyPair: 
        default: Key Pair
      LatestAmiId: 
        default: Latest cluster template AMI (created using ami_configuration_script.sh)
      NodeHibernateMaxTime:
        default: Time (seconds)

Resources:
  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
        GroupDescription: Allow SSH traffic from Internet and traffic between Slurm nodes
        VpcId: !Ref VpcId
        SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0

  SecurityGroupInbound:
      Type: AWS::EC2::SecurityGroupIngress
      Properties:
        IpProtocol: -1
        SourceSecurityGroupId: !GetAtt [ SecurityGroup, GroupId ]
        GroupId: !GetAtt [ SecurityGroup, GroupId ]

  ComputeNodeRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - ec2:DescribeTags
                Resource: '*'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

  ComputeNodeInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref ComputeNodeRole

  HeadNodeRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - ec2:AttachVolume
                  - ec2:CancelSpotInstanceRequests
                  - ec2:CreateTags
                  - ec2:DeleteTags
                  - ec2:DeleteVolume
                  - ec2:DeleteNetworkInterface
                  - ec2:DescribeLaunchTemplateVersions
                  - ec2:DescribeInstances
                  - ec2:DescribeInstanceStatus
                  - ec2:DescribeInstanceTypes
                  - ec2:DescribeSpotInstanceRequests
                  - ec2:DetachVolume
                  - ec2:ModifyNetworkInterfaceAttribute
                  - ec2:RunInstances
                  - ec2:StartInstances
                  - ec2:StopInstances
                  - ec2:TerminateInstances
                  - iam:CreateServiceLinkedRole
                  - s3:GetObject
                  - s3:GetObjectAttributes
                  - s3:ListBucket
                  - s3:PutObject
                Resource: '*'
              - Effect: Allow
                Action: iam:PassRole
                Resource: !GetAtt [ ComputeNodeRole, Arn ]
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

  HeadNodeInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref HeadNodeRole

  CleanupRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "lambda.amazonaws.com"
            Action: sts:AssumeRole
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - ec2:DescribeInstances
                  - ec2:TerminateInstances
                  - ec2:CancelSpotInstanceRequests
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvent
                Resource: '*'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

  CleanupRoleProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref CleanupRole

  HeadNodeNetworkInterface:
    Type: AWS::EC2::NetworkInterface
    Properties: 
      GroupSet: 
        - !GetAtt [ SecurityGroup, GroupId ]
      SubnetId: !Ref Subnet1Id

  ComputeNodeLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateData:
        KeyName: !Ref KeyPair
        ImageId: !Ref LatestAmiId
        EbsOptimized: true
        IamInstanceProfile:
          Arn: !GetAtt [ ComputeNodeInstanceProfile, Arn ]
        MetadataOptions:
          InstanceMetadataTags: enabled
        SecurityGroupIds:
          - !GetAtt [ SecurityGroup, GroupId ]
        UserData:
          Fn::Base64:
            !Sub |
              #!/bin/bash -x

              # As starting an instance from a snapshot doesn't actually sync the contents of that snapshot to 
              # the EBS volume, we need to initialize the volume to avoid a MASSIVE performance hit if the 
              # instance is hibernated.
              # (Even writing to free space requires the snapshot contents to be synced from S3)
              # Get the root device name
              root_dev=$(lsblk --noheadings --paths --output PKNAME $( findmnt --noheadings --output SOURCE --mountpoint / ))
              if [[ $root_dev == "" ]]; then
                root_dev =$(findmnt --noheadings --output SOURCE --mountpoint /)
              fi

              if [[ $root_dev != "" ]]; then
                fio --filename=$root_dev --rw=read --bs=10M --iodepth=32 --ioengine=libaio --direct=1 --name=volume-initialize &
              fi

              # Start NFS and mount head node directories
              sudo rm /lib/systemd/system/nfs-common.service
              systemctl daemon-reload
              systemctl enable nfs-common
              systemctl start nfs-common
              mkdir -p /nfs
              mkdir -p /shared
              sudo chmod 777 /shared/

              # Adapted from https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-old.html
              cat >> /etc/fstab << EOF
              ${HeadNodeNetworkInterface.PrimaryPrivateIpAddress}:/nfs /nfs nfs noresvport,rsize=1048576,wsize=1048576,hard,timeo=600,_netdev,x-systemd.after=network-online.target
              ${HeadNodeNetworkInterface.PrimaryPrivateIpAddress}:/shared /shared nfs noresvport,rsize=1048576,wsize=1048576,hard,timeo=600,_netdev,x-systemd.after=network-online.target
              EOF
              mount --all

              export SLURM_HOME=/nfs/slurm
              mkdir -p /var/spool/slurm
              'cp' $SLURM_HOME/etc/slurm/slurmd.service /lib/systemd/system
              systemctl enable slurmd.service
              systemctl start slurmd.service

  HeadNodeInstance:
    Type: AWS::EC2::Instance
    Properties: 
      ImageId: !Ref LatestAmiId
      InstanceType: !Ref HeadNodeInstanceType
      IamInstanceProfile: !Ref HeadNodeInstanceProfile
      KeyName:  !Ref KeyPair
      EbsOptimized: true
      NetworkInterfaces: 
        - NetworkInterfaceId: !Ref HeadNodeNetworkInterface
          DeviceIndex: 0
      Tags:
        - Key: Name
          Value: headnode
      UserData:
          Fn::Base64:
            !Sub |
              #!/bin/bash -x

              # Configure local storage (if availible).
              mountpoint=/shared/
              fdisk -l
              # NOTE: This is only tested on Ubuntu 22.04!
              localdisks=($(fdisk -l | grep -o '/dev/nvme[0-9]*n[0-9]*' | grep -v nvme0))

              if [[ ${!#localdisks[*]} -gt 0 ]]; then
                # Local storage present. Do we have multiple drives?
                if [[ ${!#localdisks[*]} -gt 1 ]]; then
                  # Multiple drives. Stripe together as RAID 0.
                  mdadm --create --verbose /dev/md0 --level 0 --raid-devices ${!#localdisks[*]} ${!localdisks[*]}
                  mountdevice="/dev/md0"
                else
                  # Single device.
                  mountdevice=${!localdisks[0]}
                fi
                # Create filesystem and mount
                mkfs.ext4 -F $mountdevice
                mount $mountdevice $mountpoint
              fi

              # Don't run host verification for instances within the subnet of this VPC.
              # Adapted from StackOverflow.
              metadata="http://169.254.169.254/latest/meta-data"
              mac=$(curl -s $metadata/network/interfaces/macs/ | head -n1 | tr -d '/')
              cidr_block=$(curl -s $metadata/network/interfaces/macs/$mac/subnet-ipv4-cidr-block)
              whitelist_ips=$(echo $cidr_block | sed 's/[.]0[.]/.*./g' | sed 's/[.]0[/].*/.*/')

              cat > /home/ubuntu/.ssh/config <<EOF
              Host $whitelist_ips
                StrictHostKeyChecking no
                UserKnownHostsFile=/dev/null
              EOF

              # Configure NFS share
              # Only export to instances on this subnet
              mkdir -p /nfs
              echo "/nfs $cidr_block(rw,async,no_subtree_check,no_root_squash,insecure)" | tee /etc/exports
              echo "$mountpoint $cidr_block(rw,async,no_subtree_check,no_root_squash,insecure)" | tee --append /etc/exports
              systemctl enable nfs-server
              systemctl start nfs-server
              exportfs -av

              export SLURM_HOME=/nfs/slurm

              cat > $SLURM_HOME/etc/aws/config.json <<EOF
              {
                  "LogLevel": "DEBUG",
                  "LogFileName": "/var/log/slurm_plugin.log",
                  "SlurmBinPath": "$SLURM_HOME/bin",
                  "Region": "${AWS::Region}",
                  "SlurmConf": {
                      "PrivateData": "CLOUD",
                     "ResumeProgram": "$SLURM_HOME/etc/aws/resume.py",
                     "SuspendProgram": "$SLURM_HOME/etc/aws/suspend.py",
                     "ResumeRate": 100,
                     "SuspendRate": 100,
                     "ResumeTimeout": 600,
                     "SuspendTime": 300,
                     "TreeWidth": 60000,
                     "GresTypes": "gres"
                  },
                  "Partitions": {
                      "compm6i": {
                          "node": {
                              "NumNodes": 100,
                              "PurchasingOption": "spot",
                              "AllocationStrategy": "price-capacity-optimized",
                              "InteruptionBehavior": "hibernate",
                              "MaxHibernationMin": 45,
                              "LaunchTemplate": "${ComputeNodeLaunchTemplate}",
                              "SubnetIds": [
                                  "${Subnet1Id}"
                              ],
                              "Instances": [
                                  "m6i.4xlarge",
                                  "m7i.4xlarge"
                              ]
                          }
                      },
                      "large": {
                          "node": {
                              "NumNodes": 100,
                              "PurchasingOption": "spot",
                              "AllocationStrategy": "price-capacity-optimized",
                              "InteruptionBehavior": "hibernate",
                              "MaxHibernationMin": 45,
                              "LaunchTemplate": "${ComputeNodeLaunchTemplate}",
                              "SubnetIds": [
                                  "${Subnet1Id}"
                              ],
                              "Instances": [
                                  "c6i.12xlarge",
                                  "c7i.12xlarge",
                                  "c7a.12xlarge",
                                  "c5.12xlarge"
                              ]
                          }
                      },
                      "xlarge": {
                          "node": {
                              "NumNodes": 4,
                              "PurchasingOption": "on-demand",
                              "AllocationStrategy": "lowest-price",
                              "InteruptionBehavior": "terminate",
                              "LaunchTemplate": "${ComputeNodeLaunchTemplate}",
                              "SubnetIds": [
                                  "${Subnet1Id}"
                              ],
                              "Instances": [
                                  "c6i.24xlarge"
                              ]
                          }
                      }
                  },
                  "PartitionOptions": {
                      "compm6i": {}
                  }
              }
              EOF

              cat > $SLURM_HOME/etc/slurm.conf <<'EOF'
              ClusterName=amazon
              ControlMachine=@HEADNODE@
              ControlAddr=${HeadNodeNetworkInterface.PrimaryPrivateIpAddress}
              SlurmdUser=root
              SlurmctldPort=6817
              SlurmdPort=6818
              AuthType=auth/munge
              StateSaveLocation=/var/spool/slurm/ctld
              SlurmdSpoolDir=/var/spool/slurm/d
              SwitchType=switch/none
              MpiDefault=none
              SlurmctldPidFile=/var/run/slurmctld.pid
              SlurmdPidFile=/var/run/slurmd.pid
              ProctrackType=proctrack/pgid
              ReturnToService=2
              # TIMERS
              SlurmctldTimeout=300
              SlurmdTimeout=${NodeHibernateMaxTime}
              InactiveLimit=0
              MinJobAge=300
              KillWait=30
              Waittime=0
              # SCHEDULING
              SchedulerType=sched/backfill
              SelectType=select/cons_res
              TaskPlugin=task/cgroup
              SelectTypeParameters=CR_Core_Memory
              DefMemPerCpu=100
              # LOGGING
              SlurmctldDebug=8
              SlurmctldLogFile=/var/log/slurmctld.log
              SlurmdDebug=8
              SlurmdLogFile=/var/log/slurmd.log
              DebugFlags=NO_CONF_HASH
              JobCompType=jobcomp/none
              EOF
              sed -i "s|@HEADNODE@|$HOSTNAME|g" $SLURM_HOME/etc/slurm.conf

              # Configure the plugin
              mkdir -p $SLURM_HOME/etc/aws/partitions/
              cd $SLURM_HOME/etc/aws/
              $SLURM_HOME/etc/aws/generate_conf.py
              cat $SLURM_HOME/etc/aws/slurm.conf.aws >> $SLURM_HOME/etc/slurm.conf
              cp $SLURM_HOME/etc/aws/gres.conf.aws $SLURM_HOME/etc/gres.conf

              crontab -l > mycron
              cat > mycron <<EOF
              * * * * * $SLURM_HOME/etc/aws/fleet_daemon.py &>/dev/null
              EOF
              crontab mycron
              rm mycron

              # Get dynamically the node name when launching slurmd
              cat > $SLURM_HOME/etc/aws/get_nodename <<'EOF'
              instanceid=`/usr/bin/curl --fail -m 2 -s http://169.254.169.254/latest/meta-data/instance-id`
              if [[ ! -z "$instanceid" ]]; then
                 hostname=`aws ec2 describe-tags --region ${AWS::Region} --filters "Name=resource-id,Values=$instanceid" "Name=key,Values=Name" --output text | cut -f5`
              fi
              if [ ! -z "$hostname" -a "$hostname" != "None" ]; then
                 echo $hostname
              else
                 echo `hostname`
              fi
              EOF
              chmod +x $SLURM_HOME/etc/aws/get_nodename

              cat > $SLURM_HOME/etc/slurm/slurmd.service <<EOF
              [Unit]
              Description=Slurm node daemon
              After=munge.service network.target remote-fs.target
              [Service]
              Type=forking
              EnvironmentFile=-/etc/sysconfig/slurmd
              ExecStartPre=/bin/bash -c "/bin/systemctl set-environment SLURM_NODENAME=\$($SLURM_HOME/etc/aws/get_nodename)"
              ExecStart=/nfs/slurm/sbin/slurmd -N \$SLURM_NODENAME \$SLURMD_OPTIONS
              ExecReload=/bin/kill -HUP \$MAINPID
              PIDFile=/var/run/slurmd.pid
              KillMode=process
              LimitNOFILE=131072
              LimitMEMLOCK=infinity
              LimitSTACK=infinity
              Delegate=yes
              [Install]
              WantedBy=multi-user.target
              EOF
              
              cat > $SLURM_HOME/etc/slurm/slurmctld.service <<EOF
              [Unit]
              Description=Slurm controller daemon
              After=network.target munge.service
              ConditionPathExists=/nfs/slurm/etc/slurm.conf
              [Service]
              Type=forking
              EnvironmentFile=-/etc/sysconfig/slurmctld
              ExecStart=/nfs/slurm/sbin/slurmctld \$SLURMCTLD_OPTIONS
              ExecReload=/bin/kill -HUP \$MAINPID
              PIDFile=/var/run/slurmctld.pid
              LimitNOFILE=65536
              [Install]
              WantedBy=multi-user.target
              EOF

              cat > $SLURM_HOME/etc/cgroup.conf <<EOF
              AllowedRAMSpace=100.0
              AllowedSwapSpace=0.0
              ConstrainRAMSpace=yes
              ConstrainSwapSpace=yes
              MemorySwappiness=0
              CgroupAutomount=yes
              ConstrainCores=yes
              EOF


              # Set environment variables
              echo 'export SLURM_HOME=/nfs/slurm' | tee /etc/profile.d/slurm.sh
              echo 'export SLURM_CONF=$SLURM_HOME/etc/slurm.conf' | tee -a /etc/profile.d/slurm.sh
              echo 'export PATH=/nfs/slurm/bin:$PATH' | tee -a /etc/profile.d/slurm.sh

              # Launch Slurmctld
              mkdir -p /var/spool/slurm
              cp /nfs/slurm/etc/slurm/slurmd.service /lib/systemd/system
              cp /nfs/slurm/etc/slurm/slurmctld.service /lib/systemd/system
              systemctl enable slurmctld
              systemctl start slurmctld

              # Add slurm to bashrc file
              echo export PATH=$SLURM_HOME/bin:'$PATH' >> /home/ubuntu/.bashrc

              cfn-signal -e $? --stack ${AWS::StackName} --resource HeadNodeInstance --region ${AWS::Region}
    CreationPolicy:
      ResourceSignal:
        Timeout: PT20M

  CleanupResources:
    Type: AWS::Lambda::Function
    Properties:
      Role:
        Fn::GetAtt:
          - CleanupRole
          - Arn
      Code:
        ZipFile:
          !Sub |
            import boto3
            import cfnresponse
            import os


            def delete():
              # Delete all EC2 instances associated with this Stack.
              region = os.environ["AWS_REGION"]
              session = boto3.session.Session(region_name=region)
              client = session.client("ec2")

              # Get all instances tagged with this fleet.
              instance_filters = [{"Name": "tag:launchtemplate", "Values": ["${ComputeNodeLaunchTemplate}"]}]
              response_describe = client.describe_instances(Filters=instance_filters)

              if len(response_describe["Reservations"]) > 0:
                for reservation in response_describe["Reservations"]:
                  for instance in reservation["Instances"]:
                    instance_id = instance["InstanceId"]

                    # Terminate the instance (one at a time, so if one instance fails to terminate it doesn't prevent the others)
                    try:
                      client.terminate_instance(InstanceIds=[instance_id])
                    except Exception:
                      pass
                    # Terminate the associated spot request (if present)
                    if "SpotInstanceRequestId" in instance:
                      spot_id = instance["SpotInstanceRequestId"]
                      try:
                        client.cancel_spot_instance_requests(SpotInstanceRequestIds=[spot_id])
                      except Exception:
                        pass

              # Get all spot requests associated with this Stack.
              instance_filters.append({"Name": "state", "Values": ["open", "active"]})
              response_describe = client.describe_spot_instance_requests(Filters=instance_filters)
              # Cancel the open spot requests
              for request in response_describe["SpotInstanceRequests"]:
                try:
                  client.cancel_spot_instance_requests(SpotInstanceRequestIds=request["SpotInstanceRequestId"])
                except Exception:
                  pass


            def handler(event, context):

              response = {}
              response_code = cfnresponse.SUCCESS
              try:
                if event["RequestType"] == "Delete":
                  delete()
                  response["output"] = "Instances terminated"
              except Exception as e:  # Failed to clean up
                reponse_code = cfnresponse.FAILED
                response["Reason"] = e
              cfnresponse.send(event, context, response_code, response)

      FunctionName:
        Fn::Join:
        - ''
        - - CleanupResources-
          - Fn::Select:
            - 2
            - Fn::Split:
              - /
              - Ref: AWS::StackId
      Handler: index.handler
      MemorySize: 128
      Runtime: python3.9
      Timeout: 120

  OnStackDeletion:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ServiceToken:
        Fn::GetAtt:
          - CleanupResources
          - Arn

Outputs:
  HeadNodeId:
    Description: Head node instance ID
    Value: !Ref HeadNodeInstance
